{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Nested cross-validation exercise\n",
    "## Nested cross-validation for feature selection with nearest neighbors <br>\n",
    "- Use Python 3 to program both a hyper-parameter selection method based on 5-fold cross-validation and a nested 5-fold cross-validation for estimating the prediction performance of models inferred with this automatic selection approach. Use base learning algorithm provided in the exercise, namely the \"use_ith_feature\" method, so that the value of the hyper-parameter i is automatically selected from the range from 1 to 100 of alternative values. The provided base learning algorithm \"use_ith_feature\" is 1-nearest neighbor that uses only the ith feature of the data given to it. The 5-fold CV based hyper-parameter selection procedure is supposed to select the best feature, e.g. the value of i, based on C-index evaluated with predictions obtained with 5-fold cross-validation. A ready-made implementation of C-index is also provided in the exercise. In nested 5-fold cross-validation, a C_index value is further evaluated on the predictions obtained from an outer 5-fold cross-validation. During each round of this outer 5-fold CV, the whole feature selection process based on inner 5-fold CV is separately done and the selected feature is used for prediction for the test data points held out during that round of the outer CV. Accordingly, the actual learning algorithm, whose prediction performance will be evaluated with nested CV, is the one that automatically selects the single best feature with 5-fold cross-validation based model selection (see the lectures and the pseudo codes presented on them for more info on this interpretation).\n",
    "- Compare the C-index produced by nested 5-fold CV with the result of ordinary 5-fold CV with the best value of i e.g. the feature providing the highest 5-fold CV C-index, and show the C-index difference between the two methods.\n",
    "- Use the provided implementation of the \"use_ith_feature\" learning algorithm and C-index functions in your exercise.\n",
    "\n",
    "As a summary, for completing this exercise implement the following steps: \n",
    "_______________________________________________________________\n",
    "#### 1. Use 5-fold cross-validation for determining the optimal i-parameter for the data (X_train.csv, y_prediction.csv) from the set of possible values of i e.g. {1,...,100}. When you have found the optimal i, save the corresponding C-index (call it 5_fold_c_index) for this parameter.\n",
    "#### 2. Similarly, use nested cross-validation ( 5-fold CV both in outer and inner folds) for estimating the C-index (call it n_5_fold_c_index) of the method that selects the best feature with 5-fold approach. \n",
    "#### 3. Return both this notebook and as a PDF-file made from it in the exercise submit page. \n",
    "_______________________________________________________________\n",
    "\n",
    "Remember to use the provided learning algorithm use_ith_feature and C-index functions in your implementation! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this cell import all libraries you need. For example: \n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provided functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "C-index function: \n",
    "- INPUTS: \n",
    "'y' an array of the true output values\n",
    "'yp' an array of predicted output values\n",
    "- OUTPUT: \n",
    "The c-index value\n",
    "\"\"\"\n",
    "def cindex(y, yp):\n",
    "    n = 0\n",
    "    h_num = 0 \n",
    "    for i in range(0, len(y)):\n",
    "        t = y[i]\n",
    "        p = yp[i]\n",
    "        for j in range(i+1, len(y)):\n",
    "            nt = y[j]\n",
    "            np = yp[j]\n",
    "            if (t != nt): \n",
    "                n = n + 1\n",
    "                if (p < np and t < nt) or (p > np and t > nt): \n",
    "                    h_num += 1\n",
    "                elif (p == np):\n",
    "                    h_num += 0.5\n",
    "    return h_num/n\n",
    "\n",
    "\"\"\"\n",
    "Self-contained 1-nearest neighbor using only a single feature\n",
    "- INPUTS: \n",
    "'X_train' a numpy matrix of the X-features of the train data points\n",
    "'y_train' a numpy matrix of the output values of the train data points\n",
    "'X_test' a numpy matrix of the X-features of the test data points\n",
    "'i' the index of the feature to be used with 1-nearest neighbor\n",
    "- OUTPUT: \n",
    "'y_predictions' a list of the output value predictions\n",
    "\"\"\"\n",
    "def use_ith_feature(X_train, y_train, X_test, i):\n",
    "    y_predictions = []\n",
    "    for test_ind in range(0, X_test.shape[0]):\n",
    "        diff = X_test[test_ind, i] - X_train[:, i]\n",
    "        distances = np.sqrt(diff * diff)\n",
    "        sort_inds = np.array(np.argsort(distances), dtype=int)\n",
    "        y_predictions.append(y_train[sort_inds[0]])\n",
    "    return y_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your implementation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell implement the required tasks\n",
    "# Read the csv files, data dose not contain headers(column names).\n",
    "# Dimention of X_train.csv is (30, 100) and for y_prediction.csv is (30, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the X_train, y_train files are read and since y_train is originally in the shape (30,) it is reshaped as (30,1) using reshape function of numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61934953, 0.89439371, 0.52379425, ..., 0.49494866, 0.41641368,\n",
       "        0.78041606],\n",
       "       [0.1290181 , 0.42088202, 0.40580428, ..., 0.96545987, 0.74954962,\n",
       "        0.12713851],\n",
       "       [0.13199532, 0.40995627, 0.52233162, ..., 0.07735719, 0.01639384,\n",
       "        0.01830257],\n",
       "       ...,\n",
       "       [0.48111007, 0.17023707, 0.40142184, ..., 0.27014721, 0.47391075,\n",
       "        0.17498264],\n",
       "       [0.55421103, 0.31098612, 0.4102353 , ..., 0.28906548, 0.23183393,\n",
       "        0.48202624],\n",
       "       [0.62422828, 0.53098756, 0.807586  , ..., 0.87799401, 0.6870305 ,\n",
       "        0.10362267]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the data ie. the csv files.\n",
    "X_train_set = np.loadtxt('X_train.csv', delimiter=',')\n",
    "\n",
    "#Reshaping y_train so that it is the appropriate shape\n",
    "y_train_set = np.loadtxt('y_prediction.csv', delimiter=',').reshape(-1,1)\n",
    "X_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36394716],\n",
       "       [0.65434878],\n",
       "       [0.37008345],\n",
       "       [0.83379345],\n",
       "       [0.19850295],\n",
       "       [0.19833447],\n",
       "       [0.30812495],\n",
       "       [0.41172255],\n",
       "       [0.7242858 ],\n",
       "       [0.1154179 ],\n",
       "       [0.58882154],\n",
       "       [0.12443519],\n",
       "       [0.65517526],\n",
       "       [0.70852902],\n",
       "       [0.75479676],\n",
       "       [0.67992716],\n",
       "       [0.95848005],\n",
       "       [0.80689364],\n",
       "       [0.83988474],\n",
       "       [0.04584075],\n",
       "       [0.13225625],\n",
       "       [0.91320714],\n",
       "       [0.4853517 ],\n",
       "       [0.44890745],\n",
       "       [0.91785166],\n",
       "       [0.07425797],\n",
       "       [0.07357497],\n",
       "       [0.14181918],\n",
       "       [0.57267871],\n",
       "       [0.90057201]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of X_train:  (30, 100)\n",
      "Dimension of y_train:  (30, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimension of X_train: \",X_train_set.shape)\n",
    "print(\"Dimension of y_train: \",y_train_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-parameter selection method based on 5-fold cross-validation and a nested 5-fold cross-validation for estimating the prediction performance of models inferred with this automatic selection approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage of 5-fold cross-validation for determining the optimal i-parameter for the data (X_train.csv, y_prediction.csv) from the set of possible values of i e.g. {1,...,100}. When you have found the optimal i, save the corresponding C-index (call it 5_fold_c_index) for this parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 5-fold CV based hyper-parameter selection procedure is supposed to select the best feature, e.g. the value of i, based on C-index evaluated with predictions obtained with 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then implementation of 5 Fold cross validation method is done from scratch using \" _5_fold_cross_validation_optimal_i\" method and only splitting of train and validation files are done using KFold.split method only. The rest is done from scratch.  Using a random state of 42, my program below shuffles the data (shuffle=True) and does K-Fold cross-validation with five folds (n_splits=5). It traverses all of the values for i between 0 and 99. Since Python is zero indexed, we essentially selected a range of potential values from 1 to 100 and executed the loop from 0 to 99. After that, K-Fold was used to split the training data into sets for each i that were used for training and validation. After splitting after each fold, the values for X_fold_train_set, X_fold_val_set, y_fold_train_set, and y_fold_val_set are acquired. The i-th feature is then retrieved using the use_ith_feature(..,..,..,..) function that was initially provided above. \n",
    "\n",
    "It trains the model on the training set and evaluates it on the validation set using the use_ith_feature function, which presumably uses the ith feature for prediction. The concordance index (cindex) for each validation set is calculated by comparing the values of the ith feature (y_pred_val_set, derived from use_ith_feature) with the y_fold_val_set. Each fold's computed mean concordance index is stored in c_indices. Finally, after identifying the ideal i by finding the index of the maximum value in c_indices, the function returns the optimum i and its associated value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for 5-fold cross-validation to find optimal i\n",
    "def _5_fold_cross_validation_optimal_i(X_train_set, y_train_set):\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    # array is kept for saving CIndices\n",
    "    c_indices = []\n",
    "    \n",
    "    for i in range(0, 100):\n",
    "        fold_c_indices = []\n",
    "        for train_idx, val_idx in kfold.split(X_train_set):\n",
    "            # Splitting into train and validation sets. It makes  number of splits(for our case 5), and selects specific indices each time for train and validation sets separately. \n",
    "            X_fold_train_set, X_fold_val_set = X_train_set[train_idx], X_train_set[val_idx]\n",
    "            y_fold_train_set, y_fold_val_set = y_train_set[train_idx], y_train_set[val_idx]\n",
    "            \n",
    "            # Retrieving the ith feature to be used\n",
    "            y_pred_val_set = use_ith_feature(X_fold_train_set, y_fold_train_set, X_fold_val_set, i)\n",
    "            # Computing the CIndex between the y_fold_val, y_pred_val\n",
    "            c_index_value = cindex(y_fold_val_set, y_pred_val_set)\n",
    "            # The CIndices are all kept in fold_c_indices to find the mean of all of them later \n",
    "            fold_c_indices.append(c_index_value)\n",
    "        \n",
    "        mean_c_index = np.mean(fold_c_indices)\n",
    "        c_indices.append(mean_c_index)\n",
    "        \n",
    "    # The optimal i is found by finding the index at which CIndex is the highest. Actual Value is found by adding 1\n",
    "    optimal_i_value = np.argmax(c_indices) + 1  \n",
    "    # Then optimal i and the appropriate Cindex is again returned. This time, the previously +1 added is removed because python is 0-indexed \n",
    "    return optimal_i_value, c_indices[optimal_i_value - 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal i is  46  and C-Index from 5-Fold Cross-Validation is:  0.74\n"
     ]
    }
   ],
   "source": [
    "# Part 1: Finding the optimal i using 5-fold cross-validation\n",
    "optimal_i_value, _5_fold_c_index = _5_fold_cross_validation_optimal_i(X_train_set, y_train_set)\n",
    "print(\"Optimal i is \",optimal_i_value,\" and C-Index from 5-Fold Cross-Validation is: \",round(_5_fold_c_index, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage of nested cross-validation ( 5-fold CV both in outer and inner folds) for estimating the C-index (call it n_5_fold_c_index) of the method that selects the best feature with 5-fold approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In nested 5-fold cross-validation, a C_index value is further evaluated on the predictions obtained from an outer 5-fold cross-validation. During each round of this outer 5-fold CV, the whole feature selection process based on inner 5-fold CV is separately done and the selected feature is used for prediction for the test data points held out during that round of the outer CV. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function uses outer 5-fold cross-validation (outer_kfold) to partition the data into training and validation sets for each fold. By splitting the training data evenly, training and validation sets (X_outer_train_set, X_outer_val_set, y_outer_train_set, y_outer_val_set) are produced for each outer fold.\n",
    "It uses the find_optimal_i function to determine the optimal value of i for the inner training data (X_outer_train_set, y_outer_train_set) and employs the use_ith_feature function with the optimal i for the outer validation set (X_outer_val_set) in order to predict the goal values (y_pred_outer_val). The concordance index (cindex), once calculated for the outer validation set (y_outer_val, y_pred_outer_val), is stored in outer_c_indices. The average concordance index over all outer folds is the end result of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for nested 5-fold cross-validation\n",
    "def nested_5_fold_cross_validation(X_train, y_train):\n",
    "    outer_kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    c_indices_outer = []\n",
    "    \n",
    "    for outer_train_idx, outer_val_idx in outer_kfold.split(X_train_set):\n",
    "        \n",
    "        # Splitting into train and validation sets. It makes  number of splits(for our case 5), and selects specific indices each time for train and validation sets separately. \n",
    "        X_outer_train_set, X_outer_val_set = X_train_set[outer_train_idx], X_train_set[outer_val_idx]\n",
    "        y_outer_train_set, y_outer_val_set = y_train_set[outer_train_idx], y_train_set[outer_val_idx]\n",
    "        \n",
    "        # The optimal i is found out from find_optimal_i function as implemented above\n",
    "        optimal_i, _ = _5_fold_cross_validation_optimal_i(X_outer_train_set, y_outer_train_set)\n",
    "        \n",
    "        # The outer validation set is found out using use_ith_feature method described earlier in this notebook\n",
    "        y_pred_outer_val = use_ith_feature(X_outer_train_set, y_outer_train_set, X_outer_val_set,  optimal_i)\n",
    "        # Then the CIndex for the outer validatin set is found between y_outer_val and y_pred_outer val\n",
    "        c_index_outer_val = cindex(y_outer_val_set, y_pred_outer_val)\n",
    "        # Resulting CIndex are kept in outer_c_indices\n",
    "        c_indices_outer.append(c_index_outer_val)\n",
    "        \n",
    "    #The mean of them are returned\n",
    "    return np.mean(c_indices_outer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C-Index from Nested 5-Fold Cross-Validation:  0.453\n"
     ]
    }
   ],
   "source": [
    "# Part 2: Nested 5-fold cross-validation\n",
    "n_5_fold_c_index = nested_5_fold_cross_validation(X_train_set, y_train_set)\n",
    "print(\"C-Index from Nested 5-Fold Cross-Validation: \",round(n_5_fold_c_index, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of the C-index produced by nested 5-fold CV with the result of ordinary 5-fold CV with the best value of i e.g. the feature providing the highest 5-fold CV C-index, and show the C-index difference between the two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in C-Index between the two methods:  0.287\n"
     ]
    }
   ],
   "source": [
    "# Part 3: Compare and show the results\n",
    "c_index_difference = _5_fold_c_index - n_5_fold_c_index\n",
    "print(\"Difference in C-Index between the two methods: \",round(c_index_difference, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
