{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student name <br> Faiza Anan Noor \n",
    "Student number 2306676 <br>\n",
    "02, 07, 2024  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 | TKO_7092 Evaluation of Machine Learning Methods 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction of the metal ion content from multi-parameter data\n",
    "<b> Use K-Nearest Neighbor Regression with euclidean distance to predict total metal concentration (c_total), concentration of Cadmium (Cd) and concentration of Lead (Pb), using number of neighbors k = 1, 3, 5, 7.</b> <br>\n",
    "\n",
    "<b> Instructions: </b> \n",
    "\n",
    "    - You may use Nearest Neighbor Regression from https://scikit-learn.org/stable/modules/neighbors.html\n",
    "    - The data should be standarized using z-score (using sklearn.preprocessing.StandardScaler is advised).\n",
    "    - Implement Leave-One-Out cross-validation and calculate the C-index for each output (c_total, Cd, Pb). \n",
    "    - Implement Leave-Replicas-Out cross-validation and calculate the C-index for each output (c_total, Cd, Pb).\n",
    "    - Explain your code by adding detailed comments. \n",
    "    - Only provide code that is relevant to the exercise.\n",
    "    - Please submit your solution as a Jupyter Notebook (.ipynb) and as a PDF file. Ensure to include your full name in the filename.     \n",
    "    - Submit to moodle your solution on ** Wednesday 7 of February ** at the latest.\n",
    "  \n",
    "<b>Please follow the instructions and note that you are expected to submit your individual solution.<br>\n",
    "Identical or overly similar submissions will result in the exercise being marked as failed.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In this cell import all libraries you need. For example: \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and visualize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mod1</th>\n",
       "      <th>Mod2</th>\n",
       "      <th>Mod3</th>\n",
       "      <th>c_total</th>\n",
       "      <th>Cd</th>\n",
       "      <th>Pb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9945</td>\n",
       "      <td>119</td>\n",
       "      <td>72335</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9596</td>\n",
       "      <td>119</td>\n",
       "      <td>110542</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10812</td>\n",
       "      <td>120</td>\n",
       "      <td>98594</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10786</td>\n",
       "      <td>117</td>\n",
       "      <td>82977</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10566</td>\n",
       "      <td>108</td>\n",
       "      <td>136416</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Mod1  Mod2    Mod3  c_total   Cd    Pb\n",
       "0   9945   119   72335        0  0.0   0.0\n",
       "1   9596   119  110542        0  0.0   0.0\n",
       "2  10812   120   98594        0  0.0   0.0\n",
       "3  10786   117   82977        0  0.0   0.0\n",
       "4  10566   108  136416       14  0.0  14.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this cell read the file Water_data.csv\n",
    "# Print the dataset dimesions (i.e. number of rows and columns)\n",
    "# Print the first 5 rows of the dataset\n",
    "\n",
    "df=pd.read_csv(\"water_data.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of rows and columns is  (268, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of rows and columns is \",df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In this cell, we standardize the dataset features by removing the mean and scaling to unit variance. \n",
    " In other words, we used z-score to scale the dataset features (Mod1, Mod2, Mod3) using StandardScaler() and fit and transformed the dataFrame and stored the updated one in df_standardized. Then we printed the 5 first samples (i.e. rows) of the scaled dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mod1</th>\n",
       "      <th>Mod2</th>\n",
       "      <th>Mod3</th>\n",
       "      <th>c_total</th>\n",
       "      <th>Cd</th>\n",
       "      <th>Pb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.972283</td>\n",
       "      <td>-0.670482</td>\n",
       "      <td>-0.358179</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.975878</td>\n",
       "      <td>-0.670482</td>\n",
       "      <td>0.259488</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.963351</td>\n",
       "      <td>-0.670394</td>\n",
       "      <td>0.066333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.963619</td>\n",
       "      <td>-0.670657</td>\n",
       "      <td>-0.186137</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.965885</td>\n",
       "      <td>-0.671447</td>\n",
       "      <td>0.677776</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Mod1      Mod2      Mod3  c_total   Cd    Pb\n",
       "0 -0.972283 -0.670482 -0.358179        0  0.0   0.0\n",
       "1 -0.975878 -0.670482  0.259488        0  0.0   0.0\n",
       "2 -0.963351 -0.670394  0.066333        0  0.0   0.0\n",
       "3 -0.963619 -0.670657 -0.186137        0  0.0   0.0\n",
       "4 -0.965885 -0.671447  0.677776       14  0.0  14.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this cell, standardize the dataset features by removing the mean and scaling to unit variance. \n",
    "# In other words, use z-score to scale the dataset features (Mod1, Mod2, Mod3) \n",
    "# Print the 5 first samples (i.e. rows) of the scaled dataset\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_subset=df[['Mod1','Mod2','Mod3']]  \n",
    "y_subset = df[['c_total', 'Cd', 'Pb']]\n",
    "\n",
    "# Fit and transform the DataFrame\n",
    "X_standardized= pd.DataFrame(scaler.fit_transform(X_subset), columns=X_subset. columns)\n",
    "df_standardized=pd.concat([X_standardized, y_subset], axis=1)\n",
    "\n",
    "# Now df_standardized contains the standardized features using Z-score normalization\n",
    "\n",
    "X = df_standardized[['Mod1','Mod2','Mod3']].values  \n",
    "y = df_standardized[['c_total', 'Cd', 'Pb']].values\n",
    "df_standardized.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C-index code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In this cell, implement the C-index function. You may use the implementation from the first exercise.\n",
    "\n",
    "def cindex(y, yp):\n",
    "    n=0\n",
    "    #n = 0\n",
    "    h_num = 0 \n",
    "    for i in range(0, len(y)):\n",
    "        t = y[i]\n",
    "        p = yp[i]\n",
    "        for j in range(i+1, len(y)):\n",
    "            nt = y[j]\n",
    "            np = yp[j]\n",
    "            if (t != nt): \n",
    "                n = n + 1\n",
    "                if (p < np and t < nt) or (p > np and t > nt): \n",
    "                    h_num += 1\n",
    "                elif (p == np):\n",
    "                    h_num += 0.5\n",
    "   \n",
    "    return h_num/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the cindex function with following values\n",
    "true_labels = np.array([-1, 1, 1, -1, 1])\n",
    "predictions = np.array([0.60, 0.80, 0.75, 0.75, 0.70])\n",
    "print(predictions.shape)\n",
    "cindx = cindex(true_labels, predictions)\n",
    "print(cindx) #For this example, a correct C-index implementation will result in 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-One-Out cross-validation\n",
    "In the following cell, write and execute your code for Leave-One-Out cross-validation using K-Nearest Neighbor Regression with k values of 1, 3, 5, and 7.<br>\n",
    "Print the corresponding Leave-One-Out C-index for c_total, Cd and Pb for each k value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "We implemented the Leave-One-Out cross validation from scratch by manually deleting just one index  from the targets and labels for training to be evaluated later on.\n",
    "Then for different values of k=1,3,5,7 we ran K nearest Neighbors using these k values and stored the c_total, Pb, Pb cindex for each of them. After each iteration, for each k value also,  we stored the predictions and answers for C_total, Pb, Cd, and then after the iteration we stored cindex for each of these variables by using their Real(Answers) and Predicted Values by also adopting the necessary shaping anf flattening. We stored the Cindices in arrays to be used later on for plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining variables and arrays for storing the cindex for c_total, Pb, Cd and arrays also to store them for plotting later\n",
    "avg_c_total_loo=0\n",
    "avg_c_cd_loo=0\n",
    "avg_c_pb_loo=0\n",
    "\n",
    "avg_c_total_loo_total=[]\n",
    "avg_c_cd_loo_total=[]\n",
    "avg_c_pb_loo_total=[]\n",
    "\n",
    "\n",
    "# Define k values\n",
    "k_values = [1, 3, 5, 7]\n",
    "\n",
    "# Perform Leave-One-Out cross-validation\n",
    "for k in k_values:\n",
    "    # Defining arrays for storing answers and predictions which contain all 3 variables at once(c_total, Cd, Pb)\n",
    "    # These will be used later on by referring to indices 0, 1, 2 to retrieve answers and predictions for c_total, Cd, Pb\n",
    "    # and then cindex will be found out for each of them using these answers and predictions for each variable and we also did necessary indices\n",
    "    answers = []\n",
    "    predictions=[]\n",
    "    for i in range(len(X)):\n",
    "       # predictions = []\n",
    "        # Split data into train and test sets (leave one out manually)\n",
    "        # Define training and testing sets using boolean indexing\n",
    "        X_train = np.delete(X, i, axis=0)  \n",
    "        # Leave out one. we delete specific index from training to be tested on later for targets and labels both\n",
    "        y_train = np.delete(y, i, axis=0)\n",
    "        \n",
    "        \n",
    "        # Create test set with only the current index\n",
    "        X_test = X[i].reshape(1, -1)\n",
    "        y_test = y[i].reshape(1, -1)\n",
    "        \n",
    "        # Train KNN regression model\n",
    "        knn_model = KNeighborsRegressor(n_neighbors=k)\n",
    "        knn_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on test set containing the ith feature\n",
    "        y_pred = knn_model.predict(X_test)\n",
    "        # Storing the predictions and answers by appropriate shaping\n",
    "        predictions.append(y_pred.reshape(-1).tolist())\n",
    "        \n",
    "        answers.append(y_test.reshape(-1).tolist())\n",
    "     \n",
    "    \n",
    "    # Calculate C-index for each output\n",
    "    # 0 index refers to c_total, 1 refers to Cd, 2 refers to Pb\n",
    "    avg_c_total_loo=cindex([ans[0] for ans in answers], [pred[0] for pred in predictions])\n",
    "    avg_cd_loo=cindex([ans[1] for ans in answers], [pred[1] for pred in predictions])\n",
    "    avg_pb_loo=cindex([ans[2] for ans in answers], [pred[2] for pred in predictions])\n",
    "    \n",
    "    # Storing the respective cindices in arrays for later use(plotting)\n",
    "    \n",
    "    avg_c_total_loo_total.append(avg_c_total_loo)\n",
    "    avg_c_cd_loo_total.append(avg_cd_loo)\n",
    "    avg_c_pb_loo_total.append(avg_pb_loo)\n",
    "\n",
    "\n",
    "    # Print results\n",
    "    print(\"K = \",k)\n",
    "    print(f\"Average C-index for c_total: {avg_c_total_loo: .3f}\")    \n",
    "    print(f\"Average C-index for Cd: {avg_cd_loo: .3f}\")\n",
    "    \n",
    "    print(f\"Average C-index for Pb: {avg_pb_loo: .3f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-Replicas-Out cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In the following cell, write and execute your code for Leave-Replicas-Out cross-validation using K-Nearest Neighbor Regression with k values of 1, 3, 5, and 7.<br>\n",
    "Print the corresponding Leave-Replicas-Out C-index for c_total, Cd and Pb for each k value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 =df_standardized[\"c_total\"].values # Array of c-total values\n",
    "y2 = df_standardized[\"Cd\"].values   # Array of Cd values\n",
    "y3 = df_standardized[\"Pb\"].values   # Array of Pb values\n",
    "y = df_standardized[['c_total', 'Cd', 'Pb']].values\n",
    "\n",
    "X = df_standardized[['Mod1','Mod2','Mod3']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using sklearn.model_selection, we carried out Leave-Replicas-Out cross-validation. With the use of LeaveOneGroupOut, a cross-validator, data may be divided into training sets with all samples excluded from one particular group by providing train/test indices.  The first step in grouping the replicas accurately is to identify the groups inside the dataset; that is, each replica should have identical values for c-total, Cd, and Pb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group replicas with equal c-total, Cd, and Pb values\n",
    "group_indices = {}\n",
    "for i, (c_total, cd, pb) in enumerate(zip(y1, y2, y3)):\n",
    "    key = (c_total, cd, pb)\n",
    "    if key not in group_indices:\n",
    "        group_indices[key] = []\n",
    "    group_indices[key].append(i)\n",
    "\n",
    "# Convert dictionary values to list of tuples\n",
    "groups = list(group_indices.values())\n",
    "groups[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group labels for the samples used while splitting the dataset into train/test set. This ‘groups’ parameter must always be specified to calculate the number of splits. We calculate this using the group indices we defined earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of group labels for LeaveOneGroupOut cross-validation\n",
    "group_labels = np.zeros(len(X))\n",
    "for idx, group_indices in enumerate(groups):\n",
    "    group_labels[group_indices] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then for different values of k=1,3,5,7 we ran K nearest Neighbors using these k values and stored the c_total, Pb, Pb cindex for each of them. After each iteration, for each k value also,  we stored the predictions and answers for C_total, Pb, Cd, and then after the iteration we stored cindex for each of these variables by using their Real(Answers) and Predicted Values by also adopting the necessary shaping and flattening. We stored the Cindices in arrays to be used later on for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aRRAYS TO STORE CINDICES FOR C_TOTAL, cd, Pb\n",
    "avg_c_total_lro_list=[]\n",
    "avg_cd_lro_list=[]\n",
    "avg_pb_lro_list=[]\n",
    "\n",
    "# Define k values\n",
    "k_values = [1, 3, 5, 7]\n",
    "\n",
    "\n",
    "# Perform Leave-One-Out cross-validation\n",
    "for k in k_values:\n",
    "    \n",
    "    # defining arrays for all answers, predictions for all variables(C_total, Pb, Cd)\n",
    "    # and also 3 different pairs arrays for storing specific results for just c_total, Pb, Cd\n",
    "    \n",
    "    answers=[]\n",
    "    predictions=[]\n",
    "    \n",
    "    predictions_ctotal=[]\n",
    "    answers_ctotal=[]\n",
    "    predictions_Cd=[]\n",
    "    answers_Cd=[]\n",
    "    predictions_Pb=[]\n",
    "    answers_Pb=[]\n",
    "\n",
    "\n",
    "    \n",
    "    logo = LeaveOneGroupOut()\n",
    "    for train_index, test_index in logo.split(X, groups=group_labels):\n",
    "        \n",
    "        # Forming Train and test sets using the indices\n",
    "        X_train, X_test = X[train_index], X[test_index]        \n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Fitting the model using the specified value of k\n",
    "               \n",
    "        knn_model = KNeighborsRegressor(n_neighbors=k)\n",
    "        knn_model.fit(X_train, y_train)\n",
    "        y_pred = knn_model.predict(X_test)\n",
    "        \n",
    "        # Storing actual answers and predictions which tontain values for all variables: c_total, Cd, Pb. \n",
    "        #This will be used to extract results(predictions and answers) for each of them in the upcoming code snippets\n",
    "        predictions.append(y_pred)\n",
    "        answers.append(y_test)\n",
    "        \n",
    "        # Appending C_total real(answers) and results(answers) by using the appropriate index \n",
    "        predictions_ctotal.append(y_pred[:,0].flatten())\n",
    "        answers_ctotal.append(y_test[:,0].flatten())\n",
    "        \n",
    "        # Appending Cd real(answers) and results(answers) by using the appropriate index \n",
    "\n",
    "        predictions_Cd.append(y_pred[:,1].flatten())\n",
    "        answers_Cd.append(y_test[:,1].flatten())\n",
    "        \n",
    "        # Appending Pb real(answers) and results(answers) by using the appropriate index \n",
    "\n",
    "        predictions_Pb.append(y_pred[:,2].flatten())\n",
    "        answers_Pb.append(y_test[:,2].flatten())\n",
    "\n",
    "\n",
    "\n",
    "    # Print results\n",
    "    print(\"K = \",k)\n",
    "    avg_c_total_lro_list.append(cindex(np.array(answers_ctotal).flatten(), np.array(predictions_ctotal).flatten()))\n",
    "    avg_cd_lro_list.append(cindex(np.array(answers_Cd).flatten(), np.array(predictions_Cd).flatten()))\n",
    "    avg_pb_lro_list.append(cindex(np.array(answers_Pb).flatten(), np.array(predictions_Pb).flatten()))\n",
    "    \n",
    "    print(f\"Average C-index for c_total: {cindex(np.array(answers_ctotal).flatten(), np.array(predictions_ctotal).flatten()): .5f}\")\n",
    "    print(f\"Average C-index for Cd: {cindex(np.array(answers_Cd).flatten(), np.array(predictions_Cd).flatten()): .5f}\")\n",
    "    print(f\"Average C-index for Pb: {cindex(np.array(answers_Pb).flatten(), np.array(predictions_Pb).flatten()): .5f}\")\n",
    "\n",
    "    print(\"     \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plot Leave-One-Out and Leave-Replicas-Out Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Note: You may plot the results as they were presented in the video lecture (refer to MOOC2-Module 2 .pptx slides)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot c_total results\n",
    "axs[0].plot(k_values, avg_c_total_loo_total, marker='o', label='LOO')\n",
    "axs[0].plot(k_values, avg_c_total_lro_list, marker='o', label='LRO')\n",
    "axs[0].set_title('C-Index vs k Value for c_total')\n",
    "axs[0].set_xlabel('k Value')\n",
    "axs[0].set_ylabel('C-Index')\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot Cd results\n",
    "axs[1].plot(k_values, avg_c_cd_loo_total, marker='o', label='LOO')\n",
    "axs[1].plot(k_values, avg_cd_lro_list, marker='o', label='LRO')\n",
    "axs[1].set_title('C-Index vs k Value for Cd')\n",
    "axs[1].set_xlabel('k Value')\n",
    "axs[1].set_ylabel('C-Index')\n",
    "axs[1].legend()\n",
    "\n",
    "# Plot Pb results\n",
    "axs[2].plot(k_values, avg_c_pb_loo_total, marker='o', label='LOO')\n",
    "axs[2].plot(k_values, avg_pb_lro_list, marker='o', label='LRO')\n",
    "axs[2].set_title('C-Index vs k Value for Pb')\n",
    "axs[2].set_xlabel('k Value')\n",
    "axs[2].set_ylabel('C-Index')\n",
    "axs[2].legend()\n",
    "\n",
    "# Set x-axis range from 0 to 1\n",
    "for ax in axs:\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlim(1, 7)  # Set y-axis limit to specific values\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of results\n",
    "#### Answer the following questions based on the results obtained\n",
    "- Which cross-validation method had more optimistic results? \n",
    "- Explain the reason for the optimistic results produced by the cross-validation method.\n",
    "- Which cross-validation method generalized better on unseen data? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this cell write your answers to the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which cross-validation method had more optimistic results? \n",
    "Leave-One-Out (LOO) cross-validation frequently produces more optimistic findings than Leave-Replicas-Out (LRO) cross-validation like in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain the reason for the optimistic results produced by the cross-validation method\n",
    "\n",
    "For each sample in the dataset, LOO cross-validation entails training the model on n −1 samples and testing it on the final sample. LRO cross-validation, on the other hand, separates the data into groups or replicas and makes sure the model is tested and trained on various subsets of the data. \n",
    "\n",
    "Because of the remarkably identical training sets that Leave-One-Out (LOO) cross-validation creates, it frequently yields more optimistic findings than Leave-Replicas-Out (LRO) cross-validation. With the exception of one sample, every training set in LOO is almost exactly the same as the whole dataset. Due to the fact that it essentially saw a similar sample during training, this high similarity might create overfitting, in which the model performs well on the test set. By splitting the data into separate replicas, LRO cross-validation, on the other hand, makes sure that the model is tested and trained on diverse subsets of the data. Less optimistic findings than with LOO are obtained because this diversity in training sets reduces overfitting and gives a more accurate assessment of model performance.\n",
    "\n",
    "Because LOO uses all available data for training, it may thus seem promising at first, but compared to LRO, which offers a more reliable evaluation of model generalization, LOO frequently yields excessively optimistic findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which cross-validation method generalized better on unseen data? Why?\n",
    "\n",
    "\n",
    "When it comes to generalizing to new data, Leave-Replicas-Out (LRO) cross-validation performs better than Leave-One-Out (LOO) cross-validation in most cases. In contrast to the extremely similar training sets in LOO, LRO provides a more diversified training set, which helps minimize overfitting. Furthermore, because LRO guarantees testing on unseen copies during training, it is less impacted by data replication bias and produces estimates of model performance on unseen data that are more accurate. By dividing the data into groups or replicas, LRO cross-validation makes sure that the model is tested and trained on various subsets of the data. This variance in training sets yields a more accurate assessment of model performance and reduces overfitting.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
